{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["## Preliminary plots[\u00b6](#Preliminary-plots)\r\n\r\nBefore diving into the receiver operating characteristic (ROC) curve, we will look at two plots that will give some context to the thresholds mechanism behind the ROC and PR curves.\r\n\r\nIn the histogram, we observe that the score spread such that most of the positive labels are binned near 1, and a lot of the negative labels are close to 0. When we set a threshold on the score, all of the bins to its left will be classified as 0's, and everything to the right will be 1's. There are obviously a few outliers, such as **negative** samples that our model gave a high score, and _positive_ samples with a low score. If we set a threshold right in the middle, those outliers will respectively become **false positives** and _false negatives_.\r\n\r\nAs we adjust thresholds, the number of positive positives will increase or decrease, and at the same time the number of true positives will also change; this is shown in the second plot. As you can see, the model seems to perform fairly well, because the true positive rate decreases slowly, whereas the false positive rate decreases sharply as we increase the threshold. Those two lines each represent a dimension of the ROC curve."], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport plotly.express as px\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\nfpr, tpr, thresholds = roc_curve(y, y_score)\n\n# The histogram of scores compared to true labels\nfig_hist = px.histogram(\n    x=y_score, color=y, nbins=50,\n    labels=dict(color='True Labels', x='Score')\n)\n\nfig_hist.show()\n\n\n# Evaluating model performance at various thresholds\ndf = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n}, index=thresholds)\ndf.index.name = \"Thresholds\"\ndf.columns.name = \"Rate\"\n\nfig_thresh = px.line(\n    df, title='TPR and FPR at every threshold',\n    width=700, height=500\n)\n\nfig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig_thresh.update_xaxes(range=[0, 1], constrain='domain')\nfig_thresh.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Basic binary ROC curve[\u00b6](#Basic-binary-ROC-curve)\r\n\r\nNotice how this ROC curve looks similar to the True Positive Rate curve from the previous plot. This is because they are the same curve, except the x-axis consists of increasing values of FPR instead of threshold, which is why the line is flipped and distorted.\r\n\r\nWe also display the area under the ROC curve (ROC AUC), which is fairly high, thus consistent with our intepretation of the previous plots."], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport plotly.express as px\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y, y_score)\n\nfig = px.area(\n    x=fpr, y=tpr,\n    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Multiclass ROC Curve[\u00b6](#Multiclass-ROC-Curve)\r\n\r\nWhen you have more than 2 classes, you will need to plot the ROC curve for each class separately. Make sure that you use a [one-versus-rest](https://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest) model, or make sure that your problem has a [multi-label](https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format) format; otherwise, your ROC curve might not return the expected results."], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nnp.random.seed(0)\n\n# Artificially add noise to make task harder\ndf = px.data.iris()\nsamples = df.species.sample(n=50, random_state=0)\nnp.random.shuffle(samples.values)\ndf.loc[samples.index, 'species'] = samples.values\n\n# Define the inputs and outputs\nX = df.drop(columns=['species', 'species_id'])\ny = df['species']\n\n# Fit the model\nmodel = LogisticRegression(max_iter=200)\nmodel.fit(X, y)\ny_scores = model.predict_proba(X)\n\n# One hot encode the labels in order to plot them\ny_onehot = pd.get_dummies(y, columns=model.classes_)\n\n# Create an empty figure, and iteratively add new lines\n# every time we compute a new class\nfig = go.Figure()\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nfor i in range(y_scores.shape[1]):\n    y_true = y_onehot.iloc[:, i]\n    y_score = y_scores[:, i]\n\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    auc_score = roc_auc_score(y_true, y_score)\n\n    name = f\"{y_onehot.columns[i]} (AUC={auc_score:.2f})\"\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n\nfig.update_layout(\n    xaxis_title='False Positive Rate',\n    yaxis_title='True Positive Rate',\n    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n    xaxis=dict(constrain='domain'),\n    width=700, height=500\n)\nfig.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## Precision-Recall Curves[\u00b6](#Precision-Recall-Curves)\r\n\r\nPlotting the PR curve is very similar to plotting the ROC curve. The following examples are slightly modified from the previous examples:"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport plotly.express as px\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["In this example, we use the [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) metric, which is an alternative scoring method to the area under the PR curve."], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\nnp.random.seed(0)\n\n# Artificially add noise to make task harder\ndf = px.data.iris()\nsamples = df.species.sample(n=30, random_state=0)\nnp.random.shuffle(samples.values)\ndf.loc[samples.index, 'species'] = samples.values\n\n# Define the inputs and outputs\nX = df.drop(columns=['species', 'species_id'])\ny = df['species']\ny_onehot = pd.get_dummies(y, columns=model.classes_)\n\n# Fit the model\nmodel = LogisticRegression(max_iter=200)\nmodel.fit(X, y)\ny_scores = model.predict_proba(X)\n\n# Create an empty figure, and iteratively add new lines\n# every time we compute a new class\nfig = go.Figure()\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\n\nfor i in range(y_scores.shape[1]):\n    y_true = y_onehot.iloc[:, i]\n    y_score = y_scores[:, i]\n\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\n    auc_score = average_precision_score(y_true, y_score)\n\n    name = f\"{y_onehot.columns[i]} (AP={auc_score:.2f})\"\n    fig.add_trace(go.Scatter(x=recall, y=precision, name=name, mode='lines'))\n\nfig.update_layout(\n    xaxis_title='Recall',\n    yaxis_title='Precision',\n    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n    xaxis=dict(constrain='domain'),\n    width=700, height=500\n)\nfig.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["## References[\u00b6](#References)\r\n\r\nLearn more about `px`, `px.area`, `px.hist`:\r\n\r\n*  [https://plot.ly/python/histograms/](https://plot.ly/python/histograms/)\r\n*  [https://plot.ly/python/filled-area-plots/](https://plot.ly/python/filled-area-plots/)\r\n*  [https://plot.ly/python/line-charts/](https://plot.ly/python/line-charts/)"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### What About Dash?[\u00b6](#What-About-Dash?)\r\n\r\n[Dash](https://dash.plot.ly/) is an open-source framework for building analytical applications, with no Javascript required, and it is tightly integrated with the Plotly graphing library.\r\n\r\nLearn about how to install Dash at [https://dash.plot.ly/installation](https://dash.plot.ly/installation).\r\n\r\nEverywhere in this page that you see `fig.show()`, you can display the same figure in a Dash application by passing it to the `figure` argument of the [`Graph` component](https://dash.plot.ly/dash-core-components/graph) from the built-in `dash_core_components` package like this:\r\n\r\n``` \r\nimport plotly.graph_objects as go # or plotly.express as px\nfig = go.Figure() # or any Plotly Express function e.g. px.bar(...)\n# fig.add_trace( ... )\n# fig.update_layout( ... )\n\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\napp = dash.Dash()\napp.layout = html.Div([\n    dcc.Graph(figure=fig)\n])\n\napp.run_server(debug=True, use_reloader=False)  # Turn off reloader if inside Jupyter\r\n```"], "cell_type": "markdown"}], "metadata": {}}